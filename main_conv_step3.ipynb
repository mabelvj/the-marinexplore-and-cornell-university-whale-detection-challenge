{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from load_and_generate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (8432, 32, 32) (8432,)\n",
      "Validation set (2811, 32, 32) (2811,)\n",
      "Test set (2811, 32, 32) (2811,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'exploration.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (8432, 32, 32, 1) (8432, 2)\n",
      "Validation set (2811, 32, 32, 1) (2811, 2)\n",
      "Test set (2811, 32, 32, 1) (2811, 2)\n"
     ]
    }
   ],
   "source": [
    "image_size = 128\n",
    "image_width = 23\n",
    "image_height = 129\n",
    "\n",
    "# ------------------\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "\n",
    "num_labels = 2\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_height, image_width, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "patch_size = 5\n",
    "depth = 6\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "stride = 2 #1 very common\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_height, image_width, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  #layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "  #    [image_height // 4 * image_width // 4 * depth, num_hidden], stddev=0.1)) #modified for variable height and width\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [(image_height // stride**2) * (image_width // stride**2) * depth, num_hidden], stddev=0.1)) #modified for variable height and width  \n",
    "  #print (layer3_weights.get_shape().as_list())\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, stride,stride, 1], padding='SAME')\n",
    "    #print (conv.get_shape().as_list())\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    #print (hidden.get_shape().as_list())\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    #print (conv.get_shape().as_list())\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    #print (hidden.get_shape().as_list())\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return (tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "  \n",
    "    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  #decaying learning rate:\n",
    "  starting_learning_rate = 0.1\n",
    "  learning_rate = tf.train.exponential_decay(starting_learning_rate, num_steps, 2001, 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 32, 32, 6]\n",
      "[20, 16, 16, 6]\n",
      "[20, 16, 16, 16]\n",
      "[20, 8, 8, 16]\n",
      "[20, 120]\n",
      "[120, 2]\n",
      "[2811, 32, 32, 6]\n",
      "[2811, 16, 16, 6]\n",
      "[2811, 16, 16, 16]\n",
      "[2811, 8, 8, 16]\n",
      "[2811, 120]\n",
      "[120, 2]\n",
      "[2811, 32, 32, 6]\n",
      "[2811, 16, 16, 6]\n",
      "[2811, 16, 16, 16]\n",
      "[2811, 8, 8, 16]\n",
      "[2811, 120]\n",
      "[120, 2]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "patch_size = 5\n",
    "depth = 6\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "stride = 1 #1 very common\n",
    "\n",
    "\n",
    "depth_1 = 6\n",
    "depth_2 = 6\n",
    "depth_3 = 16\n",
    "depth_4 = 16\n",
    "depth_5 = 120\n",
    "\n",
    "patch_size_1 = 5\n",
    "patch_size_2 = 2\n",
    "patch_size_3 = 6\n",
    "patch_size_4 = 2\n",
    "patch_size_5 = 6\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_height, image_width, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_1, patch_size_1, num_channels, 6], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([6]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_3, patch_size_3, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "\n",
    "  #layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "  #    [patch_size_5, patch_size_5, depth_4, depth_5], stddev=0.1)) #no s√© si es depth_3 o depth_4 el primer argumento\n",
    "  #layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth_5]))\n",
    "\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [(image_height // ( patch_size_2*patch_size_4)) * (image_width // (patch_size_2*patch_size_4)) * depth_3, depth_5], stddev=0.1)) #modified for variable height and width  \n",
    "  #print (layer3_weights.get_shape().as_list())\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth_5]))\n",
    "  layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "      [depth_5, num_labels], stddev=0.1))\n",
    "  layer6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    print (conv1.get_shape().as_list())\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden1, ksize=[1, patch_size_2, patch_size_2, 1], strides=[1, patch_size_2, patch_size_2, 1], \n",
    "                           padding='VALID')\n",
    "    print (pool2.get_shape().as_list())\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(pool2, layer3_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    print (conv3.get_shape().as_list())\n",
    "    hidden3 = tf.nn.relu(conv3 + layer3_biases)\n",
    "    pool4 = tf.nn.max_pool(hidden3, ksize=[1,  patch_size_4, patch_size_4 , 1], strides=[1, patch_size_4, patch_size_4, 1], \n",
    "                           padding='SAME')    \n",
    "    \n",
    "    print (pool4.get_shape().as_list())\n",
    "    \n",
    "    shape = pool4.get_shape().as_list()\n",
    "\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])    \n",
    "\n",
    "    hidden5 = tf.nn.relu(tf.matmul(reshape, layer5_weights) + layer5_biases)\n",
    "    print (hidden5.get_shape().as_list())\n",
    "    print (layer6_weights.get_shape().as_list())\n",
    "    \n",
    "    \n",
    "    return (tf.matmul(hidden5, layer6_weights) + layer6_biases)\n",
    "  \n",
    "    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  #decaying learning rate:\n",
    "  starting_learning_rate = 0.1\n",
    "  learning_rate = tf.train.exponential_decay(starting_learning_rate, num_steps, 2001, 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps:421\n",
      "Initialized\n",
      "Minibatch loss at step 0: 10.490399\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "#num_steps = 3001\n",
    "print (\"num_steps:%d\" %num_steps)\n",
    "\n",
    "valid_accuracy = []\n",
    "test_accuracy = []\n",
    "pred_step = []\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 250 == 0):\n",
    "          print('Minibatch loss at step %d: %f' % (step, l))\n",
    "          print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "          print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "        if (step % 50 == 0):\n",
    "          valid_accuracy.append(accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "          test_accuracy.append(accuracy(\n",
    "                test_prediction.eval(), test_labels))\n",
    "          pred_step.append(step)\n",
    "      print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch_size_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ce9b04125654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# Variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   layer1_weights = tf.Variable(tf.truncated_normal(\n\u001b[0;32m---> 28\u001b[0;31m       [patch_size_1, patch_size_1, num_channels, depth_1], stddev=0.1))\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mlayer1_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdepth_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   layer2_weights = tf.Variable(tf.truncated_normal(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patch_size_1' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "patch_size = 5\n",
    "depth = 6\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "stride = 1 #1 very common\n",
    "\n",
    "\n",
    "depth_1 = 6\n",
    "depth_2 = 6\n",
    "depth_3 = 16\n",
    "depth_4 = 16\n",
    "depth_5 = 120\n",
    "\n",
    "patch_size_1 = 5\n",
    "patch_size_2 = 2\n",
    "patch_size_3 = 6\n",
    "patch_size_4 = 2\n",
    "patch_size_5 = 6\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_height, image_width, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_1, patch_size_1, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth_1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_2, patch_size_2, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_3, patch_size_3, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_4, patch_size_4, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size_4, patch_size_4, depth_4, depth_5], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "\n",
    "  layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "      [(image_height // (stride**2*pooling**2)) * (image_width // (stride**2*pooling**2)) * depth_6, num_hidden], stddev=0.1)) #modified for variable height and width  \n",
    "\n",
    "  layer6_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='VALID')\n",
    "    print (conv1.get_shape().as_list())\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden1, ksize=[1, depth_2, depth_2, 1], strides=[1, pooling, pooling, 1], padding='SAME')\n",
    "    print (pool2.get_shape().as_list())\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, stride, stride, 1], padding='VALID')\n",
    "    print (conv2.get_shape().as_list())\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1], strides=[1, pooling, pooling, 1], padding='SAME')    \n",
    "    print (pool2.get_shape().as_list())\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])    \n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "    return (tf.matmul(hidden3, layer4_weights) + layer4_biases)\n",
    "  \n",
    "    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  #decaying learning rate:\n",
    "  starting_learning_rate = 0.1\n",
    "  learning_rate = tf.train.exponential_decay(starting_learning_rate, num_steps, 2001, 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f0d19bead198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFkCAYAAACuFXjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAFUpJREFUeJzt3X+s5XV95/HXG7A1GjsJZZeBQEQTrbRNh8y1iQHU+sdI\nadTVhEqvJdqxNgLbmr2tm27iqjFuS1ormG4M0xKhGPW2+MeKbWOBrtZooS7cSdtlS6mFxRWtY9R1\nXBUqynv/OHf0cjP3M3PO3HsuM/N4JCdyv+f743M+Xu558j3fc051dwAANnLKdg8AAHhyEwsAwJBY\nAACGxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IBABgSCwDA0NSxUFUvrKqPVtUXqurxqnrFUWzz4qq6\np6oeqap/rqo3zjZcAGDeZjmz8PQkf5vk6iRH/GKJqjovyZ8n+WSSC5Jck+T3q+pVMxwbAJizOpYv\nkqqqx5O8srs/Oljnd5K8rLt/Ys2y65P8VHdfNPPBAYC5mMc1Cy9Icvu6ZbcleX5VnTqH4wMAx+C0\nORxjZ5ID65YdWD32GYe5L1X1o0kuSfJQkke3eHwAcCJ5apLzktzW3V/djB3OIxZmcUmSD273IADg\nOPaLST60GTuaRyx8KcmZ65admeS7Sb6ywTYPJckHPvCBnH/++Vs3Mp5gaWkp11133XYP46RizufP\nnM+fOZ+v++67L1dccUWy+ly6GeYRC3cledm6ZZckuae7v7fBNo8myfnnn5/du3dv5dhYY8eOHeZ7\nzsz5/Jnz+TPn22bTXsaf5XMWnl5Vu6rqgtVFz179+dzV+6+pqpvXbLIvyTOr6t1V9byqen2SvUne\ndcyjBwC23CxnFp6f5BOZfMZCJ3n36vKbk7w+kwsazz20cnc/VFU/l+S6TD6b4YtJfq27P3IM4wYA\n5mTqWOjuT2ZwRqK79x5m2acyiQwA4DjjuyH4vsXFxe0ewknHnM+fOZ8/c378O6ZPcNwqVbU7ycrK\nyoqLYgBgCvv378/CwkKSLHT3/s3YpzMLAMCQWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAENiAQAY\nEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAM\niQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACG\nxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABD\nYgEAGBILAMDQTLFQVVdX1YNV9UhV3V1VFx9h/ddW1d9V1beq6otVdWNVnT7bkAGAeZo6Fqrq8iTX\nJXlnkguSfDrJx6rqnA3W/5kkNyb5wyQ/nuSyJD+d5IbZhgwAzNMsZxaWktzQ3Td19/3dvZTk80mu\n2mD9hST/u7vf292f6+47k/xBkufPNmQAYJ6mioWqekomT/53rLvr9iQXbrDZHUnOrKpLV/dxZpKf\nT/Jn0w0VANgO055ZOCPJqUkOrFt+IMnOw23Q3X+f5LVJPlxV30nyL0m+luRNUx4bANgGp231Aarq\nBUluTvK2TM5AnJXk9zJ5KeINo22XlpayY8eOJyxbXFzM4uLi1gwWAI4jy8vLWV5efsKygwcPbvpx\nqruPfuXJyxDfTnJZd9+6Zvl7kuzq7pccZps/TnJKd796zbKLknwqyVndvf4sRapqd5KVlZWV7N69\ne5rHAwAntf3792dhYSFJFrp7/2bsc6qXIbr7sSQrSfasu2tPkjsHx/juumWPJ+kkNc3xAYD5m+Vl\niGuTvL+qVpLcleSNSc5Ncn2SVNU1Sc7u7tetrv+RJDdV1ZVJbktydiZvvfxMd3/pGMcPAGyxqWOh\nu29Z/UClt2Zy/cG9SS7t7odXV9mZSTwcWv9DVfUjSf59JtcqfD3Jf0/yn45x7ADAHMx0gWN370uy\nb4P79k6zPgDw5Oa7IQCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkF\nAGBILAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQC\nADAkFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IB\nABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAkFgCAIbEA\nAAyJBQBgaKZYqKqrq+rBqnqkqu6uqouPsP4PVdVvVdVDVfVoVX22qn5pphEDAHN12rQbVNXlSa5L\ncmWSO1f/92NVdX53P7zBZh9O8m+S7E3yQJJ/O8uxAYD5m+UJeynJDd1906Gfq+qSJFclecv6lavq\nZ5O8MMmzu/vrq4v/zyyDBQDmb6qXIarqKUkWktyx7q7bk1y4wWYvT3JPkt+sqoer6v6qeldVPXXq\n0QIAczftmYUzkpya5MC65QeS7Nxgm2dncmbh0SSvXN3H9UlOT/LLUx4fAJizeVw3cEqSx5O8pru/\nmSRV9etJPlxVV3f3v2604dLSUnbs2PGEZYuLi1lcXNzK8QLAcWF5eTnLy8tPWHbw4MFNP05199Gv\nPHkZ4ttJLuvuW9csf0+SXd39ksNs80dJLuzu565Z9rwk/yvJc7v7gcNsszvJysrKSnbv3j3FwwGA\nk9v+/fuzsLCQJAvdvX8z9jnVNQvd/ViSlSR71t21J5N3RhzOXyc5u6qetmbZj2VytmGjd08AAE8S\ns3zOwrVJ3lBVe6vqeVV1XZJzM7kOIVV1TVXdvGb9DyX5apKbqur8qnpRkt9N8r7RSxAAwJPD1Ncs\ndPctVXV6krcmOSvJvUkuXfMZCzsziYdD63+rqvYk+a9J7s4kHP5kdXsA4Elupgscu3tfkn0b3Lf3\nMMv+KcklsxwLANhevhsCABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQ\nWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBI\nLAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAk\nFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IBABgS\nCwDAkFgAAIZmioWqurqqHqyqR6rq7qq6+Ci3u6iqHquq/bMcFwCYv6ljoaouT3JdkncmuSDJp5N8\nrKrOOcJ2O5LcnOQvZxgnALBNZjmzsJTkhu6+qbvv7+6lJJ9PctURttuX5INJ/maGYwIA22SqWKiq\npyRZSHLHurtuT3LhYLu9SZ6V5B3TDhAA2F6nTbn+GUlOTXJg3fIDSXYeboOqek6S305ycXc/XlVT\nDxIA2D7TxsJUquqUTF56eHt3P3Bo8dFuv7S0lB07djxh2eLiYhYXFzdvkABwnFpeXs7y8vITlh08\neHDTj1PdffQrT16G+HaSy7r71jXL35NkV3e/ZN36O5L83yTfzQ8i4ZTVf/5ukpd2918d5ji7k6ys\nrKxk9+7dUz0gADiZ7d+/PwsLC0my0N2b8u7Dqa5Z6O7Hkqwk2bPurj1J7jzMJt9I8pOZvGti1+pt\nX5J/XP3nz0w5XgBgzmZ5GeLaJO+vqpUkdyV5Y5Jzk1yfJFV1TZKzu/t1PTlt8Q9rN66qLyd5tLvv\nO6aRAwBzMXUsdPctVXV6krcmOSvJvUku7e6HV1fZmUk8AAAngJkucOzufZm8nHC4+/YeYdt3xFso\nAeC44bshAIAhsQAADIkFAGBILAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABD\nYgEAGBILAMCQWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAh\nsQAADIkFAGBILAAAQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQ\nWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBo\nplioqqur6sGqeqSq7q6qiwfrvqqqbq+qL1fVwaq6s6peOvuQAYB5mjoWquryJNcleWeSC5J8OsnH\nquqcDTZ5UZLbk1yaZHeSjyf506raNdOIAYC5muXMwlKSG7r7pu6+v7uXknw+yVWHW7m7l7r797p7\npbsf6O7/nOSzSV4++7ABgHmZKhaq6ilJFpLcse6u25NceJT7qCTPSPK1aY4NAGyPac8snJHk1CQH\n1i0/kGTnUe7jzUmeluSWKY8NAGyD0+Z5sKpaTPK2JK/o7q8caf2lpaXs2LHjCcsWFxezuLi4RSME\ngOPH8vJylpeXn7Ds4MGDm36c6u6jX3nyMsS3k1zW3beuWf6eJLu6+yWDbS9P8r7Vbf/iCMfZnWRl\nZWUlu3fvPurxAcDJbv/+/VlYWEiShe7evxn7nOpliO5+LMlKkj3r7tqT5M6Ntls9o3Bjkl84UigA\nAE8us7wMcW2S91fVSpK7krwxyblJrk+Sqromydnd/brVn1+T5I+SvCnJ3VV15up+Hunubxzb8AGA\nrTZ1LHT3LVV1epK3Jjkryb1JLu3uh1dX2ZlJPBzyK5lcFPne1dshNyd5/SyDBgDmZ6YLHLt7X5J9\nG9y3d93PG17HAAA8+fluCABgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBILAAA\nQ2IBABgSCwDAkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAkFgCA\nIbEAAAyJBQBgSCwAAENiAQAYEgsAwJBYAACGxAIAMCQWAIAhsQAADIkFAGBILAAAQ2IBABgSCwDA\nkFgAAIbEAgAwJBYAgCGxAAAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAkFgCAIbEAAAyJBQBg\nSCwAAENige9bXl7e7iGcdMz5/Jnz+TPnx7+ZYqGqrq6qB6vqkaq6u6ouPsL6L66qe1bX/+eqeuNs\nw2Ur+Rd6/sz5/Jnz+TPnx7+pY6GqLk9yXZJ3JrkgyaeTfKyqztlg/fOS/HmST66uf02S36+qV802\nZABgnmY5s7CU5Ibuvqm77+/upSSfT3LVButfleRz3f0bq+u/L8mNSd4825ABgHmaKhaq6ilJFpLc\nse6u25NcuMFmL1i9f63bkjy/qk6d5vgAwPydNuX6ZyQ5NcmBdcsPJNm5wTY7N1j/tNX9rb8vSZ6a\nJPfdd9+Uw+NYHDx4MPv379/uYZxUzPn8mfP5M+fztea586mbtc9pY2FezkuSK664YpuHcfJZWFjY\n7iGcdMz5/Jnz+TPn2+K8JHduxo6mjYWvJPlekjPXLT8zyZc22OZLG6z/3dX9Hc5tSX4xyUNJHp1y\njABwMntqJqFw22btcKpY6O7HqmolyZ4kt665a0+Sj2yw2V1JXrZu2SVJ7unu721wnK8m+dA0YwMA\nvm9TzigcMsu7Ia5N8oaq2ltVz6uq65Kcm+T6JKmqa6rq5jXr70vyzKp69+r6r0+yN8m7jnXwAMDW\nm/qahe6+papOT/LWJGcluTfJpd398OoqOzOJh0PrP1RVP5fJZzNcneSLSX6tuzc6EwEAPIlUd2/3\nGACAJzHfDQEADIkFAGBoW2LBF1HN3zRzXlWvqqrbq+rLVXWwqu6sqpfOc7wngml/z9dsd1FVPVZV\nPsVmSjP8bfmhqvqtqnqoqh6tqs9W1S/NabgnhBnm/LVV9XdV9a2q+mJV3bh6HRxHoapeWFUfraov\nVNXjVfWKo9jmmJ9D5x4Lvohq/qad8yQvyuQjui9NsjvJx5P8aVXtmsNwTwgzzPmh7XYkuTnJX275\nIE8wM875h5O8JJN3aD03yWKSf9zioZ4wZvh7/jOZfDfQHyb58SSXJfnpJDfMY7wniKcn+dtM3jBw\nxIsON+s5dO4XOFbV32TyGQu/umbZPyT5b939lsOs/ztJXtbdP7Fm2fVJfqq7L5rHmI930875Bvu4\nN8kfd/d/2aJhnlBmnfOqWk7yT0keT/Lvunv3lg/2BDHD35afzeTzXJ7d3V+f30hPHDPM+W8kubK7\nn7Nm2a8m+Y/d/cx5jPlEUlWPJ3lld390sM6mPIfO9cyCL6KavxnnfP0+Kskzknxtc0d3Ypp1zqtq\nb5JnJXnH1o3uxDTjnL88yT1JfrOqHq6q+6vqXVW1aZ+nfyKbcc7vSHJmVV26uo8zk/x8kj/bqnGy\nOc+h8/5uiHl9ERU/MMucr/fmJE9LcssmjutENvWcV9Vzkvx2kou7+/FJnzGFWX7Pn53khZl8pPwr\nV/dxfZLTk/zy1gzzhDL1nHf331fVa5N8uKp+KJO/47cmedNWDvQktynPod4NwVBVLSZ5W5JXd/dG\n3+XBMaiqU5J8MMnbu/uBQ4u3cUgni1MyebnnNd19T3f/RZJfT/K6qvrh7R3aiamqXpDJNTlvy+R6\nqEsyibY/2M5xcWTzPrMwry+i4gdmmfMk37946YYkl3X3J7ZmeCekaef8GUmen+SCqnrv6rJTMnkF\n6DtJXtrdf7VFYz1RzPJ7/i9JvtDd31yz7L5MQu2cJA8cdisOmWXO/0OS27r72tWf762qq5N8qqre\n0t3OFG++TXkOneuZhe5+LMmhL6Jaa082/tKLuw6z/vCLqPiBGef80BmFG5P8wup/cXGUZpjzbyT5\nyUyuVN61etuXyVX5u5J8ZssGe4KY8ff8r5OcXVVPW7PsxzI52/Dw4TfhkBnn/JRMnqTWejyTq/qd\nTdsam/Mc2t1zvSV5dSavEe5N8rxM3nbzjSTnrN5/TZKb16x/XpL/l+Tdq+u/fnX7V8577MfrbYY5\nf02S7yS5MpMCPXT7ke1+LMfLbdo5P8z2b0+yf7sfx/F0m+H3/OlJPpfkT5Kcn8lbhu9Psm+7H8vx\ncpvxb8u/rv5teVaSi5L8jyR3bvdjOV5uq7+3uzL5j4vHMzlbsyvJuRvM+aY8h27Xg70yyYNJHkly\nd5KL1tx3U5KPr1v/hZlctfxIJqcGf2W7/w873m7TzHmST2RyenH97cbtfhzH023a3/N124qFOcx5\nJp+tcFuSb66Gw+8m+eHtfhzH022GOb8yyf9cnfOHM7mG4aztfhzHyy3Ji1cj4bB/n7fqOdQXSQEA\nQ94NAQAMiQUAYEgsAABDYgEAGBILAMCQWAAAhsQCADAkFgCAIbEAAAyJBQBgSCwAAEP/H3UNYGQv\n7n+zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bcd374310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.plot(pred_step,valid_accuracy, label= 'validation')\n",
    "plt.plot(pred_step,test_accuracy, label= 'test')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 32, 32, 6]\n",
      "[20, 16, 16, 6]\n",
      "[20, 4, 4, 6]\n",
      "[20, 2, 2, 6]\n",
      "[2811, 32, 32, 6]\n",
      "[2811, 16, 16, 6]\n",
      "[2811, 4, 4, 6]\n",
      "[2811, 2, 2, 6]\n",
      "[2811, 32, 32, 6]\n",
      "[2811, 16, 16, 6]\n",
      "[2811, 4, 4, 6]\n",
      "[2811, 2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 2\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "stride = 2\n",
    "\n",
    "\n",
    "#---------------------\n",
    "batch_size = 20\n",
    "patch_size = 4\n",
    "depth = 6\n",
    "num_hidden = 32\n",
    "num_channels = 1\n",
    "stride = 4\n",
    "pooling = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_height, image_width, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  #layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "  #    [image_height // 4 * image_width // 4 * depth, num_hidden], stddev=0.1)) #modified for variable height and width\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [(image_height // (stride**2*pooling**2)) * (image_width // (stride**2*pooling**2)) * depth, num_hidden], stddev=0.1)) #modified for variable height and width  \n",
    "\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='VALID')\n",
    "    print (conv1.get_shape().as_list())\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(hidden1, ksize=[1, 2, 2, 1], strides=[1, pooling, pooling, 1], padding='SAME')\n",
    "    print (pool1.get_shape().as_list())\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, stride, stride, 1], padding='VALID')\n",
    "    print (conv2.get_shape().as_list())\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1], strides=[1, pooling, pooling, 1], padding='SAME')    \n",
    "    print (pool2.get_shape().as_list())\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])    \n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "    return (tf.matmul(hidden3, layer4_weights) + layer4_biases)\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.3).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps:421\n",
      "Initialized\n",
      "Minibatch loss at step 0: 1.215984\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 250: 0.514865\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.2%\n",
      "Test accuracy: 78.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = train_dataset.shape[0] // batch_size\n",
    "\n",
    "#num_steps = 3001\n",
    "print (\"num_steps:%d\" %num_steps)\n",
    "\n",
    "valid_accuracy = []\n",
    "test_accuracy = []\n",
    "pred_step = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    #do not randomize train dataset each step: it is learning from slices and it will go mad!\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    #print (\"offset %d\" %offset)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 250 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    if (step % 250 == 0):\n",
    "      valid_accuracy.append(accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      test_accuracy.append(accuracy(\n",
    "            test_prediction.eval(), test_labels))\n",
    "      pred_step.append(step)\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7cf56cfc10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn9fi/qzx7KTvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7db801ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.plot(pred_step,valid_accuracy, label= 'validation')\n",
    "plt.plot(pred_step,test_accuracy, label= 'test')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-7826120aa0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#do not randomize train dataset each step: it is learning from slices and it will go mad!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_array' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "valid_accuracy = []\n",
    "test_accuracy = []\n",
    "pred_step = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for batch_size in batch_array:\n",
    "      for step in range(num_steps):\n",
    "        #do not randomize train dataset each step: it is learning from slices and it will go mad!\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "          print('Minibatch loss at step %d: %f' % (step, l))\n",
    "          print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "          print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "          valid_accuracy.append(accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "          test_accuracy.append(accuracy(\n",
    "            test_prediction.eval(), test_labels))\n",
    "          pred_step.append(step)\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
